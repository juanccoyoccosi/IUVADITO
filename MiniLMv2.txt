# NLP y procesamiento sem√°ntico
import nltk
from nltk.stem.lancaster import LancasterStemmer
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cosine

# IA y modelos
import google.generativeai as genai
import tensorflow as tf

# Telegram Bot
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext

# Utilidades generales
import numpy as np
import json
import random
import logging
import os
import pickle
import subprocess
import difflib
import requests
from typing import Tuple

#  Base de datos
import couchdb
from flask import Flask, request, jsonify
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--port", type=int, default=5000)
args = parser.parse_args()


class IntentClassifier:
    def __init__(self, db, campo_texto="patrones", campo_embeddings="embedding"):
        self.modelo = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
        self.intent_embeddings = {}
        self.db = db
        
        # Configurar Gemini API (obt√©n tu API key gratis en https://makersuite.google.com/app/apikey)
        GEMINI_API_KEY = 'AIzaSyBE535gpiIYwV58b0grSsZ4WH4yuk_r4xQ'
        genai.configure(api_key=GEMINI_API_KEY)
        
        # Usar el modelo gratuito gemini
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        
        print(f"‚úì Gemini API configurada correctamente")
        
        # Generar embeddings para documentos que no los tengan
        for doc_id in db:
            doc = db[doc_id]
            
            if campo_embeddings in doc:
                print(f"Documento {doc_id} ya tiene embedding")
                self.intent_embeddings[doc_id] = doc[campo_embeddings]
                continue
                
            if campo_texto in doc:
                texto = doc.get(campo_texto)
                embedding = self.modelo.encode([texto])[0].tolist()
                doc[campo_embeddings] = embedding
                db.save(doc)
                self.intent_embeddings[doc_id] = embedding
                print(f"Embedding generado para {doc_id}")
            else:
                print(f"Documento {doc_id} no tiene el campo '{campo_texto}'")

    def classify_with_rag(self, query: str, threshold: float = 0.3) -> Tuple[str, float, str]:
        """Clasifica la intenci√≥n y SIEMPRE genera respuesta usando RAG con Gemini"""
        query_embedding = self.modelo.encode([query])[0]
        
        best_intent = "None"
        best_score = -1
        best_doc = None
        
        # Encontrar el documento m√°s similar
        for intent, intent_embedding in self.intent_embeddings.items():
            similarity = np.dot(query_embedding, intent_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(intent_embedding)
            )
            
            if similarity > best_score:
                best_score = similarity
                best_intent = intent
                best_doc = self.db[best_intent]
        
        # SIEMPRE generar respuesta con Gemini usando RAG
        if best_score < threshold:
            respuesta_gemini = self.generate_response_with_gemini_unknown(query, best_doc)
            return "unknown", best_score, respuesta_gemini
        else:
            respuesta_gemini = self.generate_response_with_gemini(query, best_doc)
            return best_intent, best_score, respuesta_gemini

    def generate_response_with_gemini(self, query: str, context_doc: dict) -> str:
        """Genera respuesta usando Gemini con el contexto recuperado"""
        
        # Preparar el contexto del documento
        context_info = []
        if 'patrones' in context_doc:
            context_info.append(f"Patrones relacionados: {context_doc['patrones']}")
        if 'respuestas' in context_doc:
            context_info.append(f"Informaci√≥n relevante: {context_doc['respuestas']}")
        if 'categoria' in context_doc:
            context_info.append(f"Categor√≠a: {context_doc['categoria']}")
        
        context_text = "\n".join(context_info)
        
        # Prompt para Gemini
        prompt = f"""Eres un asistente inteligente y √∫til. Tu tarea es responder preguntas bas√°ndote en la informaci√≥n proporcionada en el contexto.

Instrucciones:
- Usa √∫nicamente la informaci√≥n del contexto para responder
- Si la informaci√≥n no es suficiente, di que necesitas m√°s detalles
- Responde de manera clara, concisa y amigable
- Mant√©n un tono profesional pero cercano
- Si hay m√∫ltiples opciones o respuestas posibles, menci√≥nalas todas

Contexto relevante:
{context_text}

Pregunta del usuario: {query}

Por favor, responde bas√°ndote en el contexto proporcionado."""

        return self._call_gemini_api(prompt)

    def generate_response_with_gemini_unknown(self, query: str, best_doc: dict) -> str:
        """Genera respuesta para consultas con baja similitud usando RAG"""
        
        # Usar el mejor documento encontrado aunque la similitud sea baja
        context_info = ["Informaci√≥n disponible en la base de conocimientos:"]
        if best_doc:
            if 'patrones' in best_doc:
                context_info.append(f"Patrones: {best_doc['patrones']}")
            if 'respuestas' in best_doc:
                context_info.append(f"Informaci√≥n: {best_doc['respuestas']}")
            if 'categoria' in best_doc:
                context_info.append(f"Categor√≠a: {best_doc['categoria']}")
        
        context_text = "\n".join(context_info)
        
        # Prompt espec√≠fico para consultas unclear
        prompt = f"""Eres un asistente inteligente. El usuario ha hecho una consulta que no coincide exactamente con la informaci√≥n disponible, pero debes intentar ayudar.

Instrucciones:
- Usa la informaci√≥n del contexto si es relevante
- Si no hay informaci√≥n relevante, pide amablemente m√°s detalles espec√≠ficos
- Sugiere temas relacionados si los hay
- Mant√©n un tono amigable y √∫til
- No inventes informaci√≥n que no est√© en el contexto

{context_text}

Consulta del usuario: {query}

La consulta no tiene una coincidencia exacta. Ayuda al usuario bas√°ndote en la informaci√≥n disponible o pide m√°s detalles."""

        return self._call_gemini_api(prompt)

    def _call_gemini_api(self, prompt: str) -> str:
        """Llama a la API de Gemini y retorna la respuesta"""
        try:
            response = self.gemini_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=500,
                )
            )

            # Validar que Gemini devolvi√≥ contenido
            if hasattr(response, "parts") and response.parts:
                return response.text
            else:
                print("‚ö†Ô∏è Gemini no devolvi√≥ contenido v√°lido.")
                finish_reason = response.candidates[0].finish_reason if response.candidates else "desconocido"
                print(f"üìç Gemini finish_reason: {finish_reason}")
                return "No pude generar una respuesta clara. ¬øPodr√≠as reformular tu pregunta?"

        except Exception as e:
            print(f"‚ùå Error conectando con Gemini API: {e}")
            return "Error: Servicio de IA no disponible. Por favor, verifica tu API key de Gemini."

app = Flask(__name__)

# Inicializar el clasificador una vez
USERNAME = "Juan"
PASSWORD = "Elpro123"
DATABASE = "chatbot_data"
couch = f"http://{USERNAME}:{PASSWORD}@62.171.179.255:5984/"
server = couchdb.Server(couch)
db = server[DATABASE]
classifier = IntentClassifier(db)

@app.route('/bot', methods=['POST'])
def responder():
    try:
        data = request.json
        mensaje = data.get("body", {}).get("data", {}).get("message", {}).get("conversation", "")
        nombre = data.get("body", {}).get("data", {}).get("pushName", "Usuario")
        
        if not mensaje:
            return jsonify({
                "respuesta": "No recib√≠ ning√∫n mensaje.",
                "estado": "ERROR"
            })
        
        print(f"üì© Procesando mensaje de {nombre}: {mensaje}")
        
        # Usar el clasificador con RAG para responder
        intent, score, respuesta = classifier.classify_with_rag(mensaje)
        
        print(f"‚úì Respuesta generada: {respuesta[:100]}...")
        
        return jsonify({
            "text": respuesta,
            "estado": "OK"
        })
        
    except Exception as e:
        print(f"‚ùå Error procesando solicitud: {e}")
        return jsonify({
            "respuesta": "Error interno del servidor. Intenta nuevamente.",
            "estado": "ERROR"
        })

if __name__ == '__main__':
    context = ('server.crt', 'server.key')  # Ruta relativa o absoluta
    app.run(host='0.0.0.0', port=args.port, ssl_context=context)